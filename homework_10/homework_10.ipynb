{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-Negative  Matrix  Factorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.externals import joblib\n",
    "from sklearn import metrics\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_20 = fetch_20newsgroups(remove=('headers', 'footers', 'quotes'), random_state=74)\n",
    "texts = docs_20.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 11314 raw text documents\n"
     ]
    }
   ],
   "source": [
    "raw_documents = []\n",
    "snippets = []\n",
    "for t in texts:\n",
    "    text = t.strip()\n",
    "    raw_documents.append( text.lower() )\n",
    "        \n",
    "    snippets.append( text[0:min(len(text),100)] )\n",
    "print(\"Read %d raw text documents\" % len(raw_documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopword list has 350 entries\n"
     ]
    }
   ],
   "source": [
    "# custom stopwords\n",
    "custom_stop_words = []\n",
    "with open( \"stopwords.txt\", \"r\" ) as f:\n",
    "    for line in f.readlines():\n",
    "        custom_stop_words.append( line.strip().lower() )\n",
    "        \n",
    "print(\"Stopword list has %d entries\" % len(custom_stop_words) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 11314 X 6004 TF-IDF-normalized document-term matrix\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['articles-tfidf.pkl']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create BoW + tf-idf model\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words=custom_stop_words, min_df = 20)\n",
    "A = vectorizer.fit_transform(raw_documents)\n",
    "print( \"Created %d X %d TF-IDF-normalized document-term matrix\" % (A.shape[0], A.shape[1]) )\n",
    "\n",
    "terms = vectorizer.get_feature_names()\n",
    "joblib.dump((A,terms,snippets), \"articles-tfidf.pkl\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "know (183.69)\n",
      "get (172.95)\n",
      "people (160.03)\n",
      "think (156.97)\n",
      "good (133.11)\n",
      "thanks (133.07)\n",
      "time (127.37)\n",
      "please (118.39)\n",
      "edu (108.17)\n",
      "see (102.61)\n",
      "god (101.83)\n",
      "need (101.57)\n",
      "way (100.23)\n",
      "want (98.14)\n",
      "right (97.48)\n",
      "problem (95.40)\n",
      "system (94.64)\n",
      "windows (93.58)\n",
      "something (90.52)\n",
      "really (89.58)\n"
     ]
    }
   ],
   "source": [
    "# top features from tf-idf model\n",
    "import operator\n",
    "\n",
    "\n",
    "sums = np.array(A.sum(axis=0)).ravel()\n",
    "# map weights to the terms\n",
    "weights = { term: sums[col] for col, term in enumerate(terms)}\n",
    "ranking = sorted(weights.items(), key=operator.itemgetter(1), reverse=True)\n",
    "for i, pair in enumerate( ranking[0:20] ):\n",
    "    print( \"%s (%.2f)\" % ( pair[0], pair[1] ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "(A,terms,snippets) = joblib.load( \"articles-tfidf.pkl\")\n",
    "\n",
    "# create the model\n",
    "k = 20\n",
    "model = NMF( init=\"nndsvd\", n_components=k ) \n",
    "\n",
    "W = model.fit_transform( A )\n",
    "H = model.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11314, 20)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.07, 0.  ,\n",
       "       0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W[0,:].round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 6004)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 01: good, time, see, back, really, got, go, something, thing, long, take, going, way, bike, problem, want, sure, little, work, right\n",
      "Topic 02: thanks, please, mail, advance, looking, hi, email, info, address, send, post, help, information, someone, appreciated, interested, anybody, reply, list, find\n",
      "Topic 03: geb, cadre, dsl, chastity, n3jxp, shameful, pitt, intellect, skepticism, surrender, gordon, banks, soon, edu, blood, patients, probably, medical, weight, disease\n",
      "Topic 04: god, jesus, bible, christ, believe, faith, christian, christians, church, life, sin, truth, heaven, lord, hell, belief, man, christianity, love, father\n",
      "Topic 05: key, chip, encryption, clipper, keys, escrow, system, algorithm, government, nsa, security, public, secure, phone, bit, des, encrypted, chips, secret, number\n",
      "Topic 06: drive, scsi, drives, disk, hard, ide, controller, floppy, cd, mac, hd, system, boot, tape, cable, internal, rom, seagate, mb, computer\n",
      "Topic 07: windows, file, dos, files, program, version, running, ftp, pc, directory, run, os, microsoft, system, software, drivers, nt, disk, driver, problem\n",
      "Topic 08: 00, sale, 10, shipping, offer, price, 50, 20, condition, 15, asking, 25, 12, 30, 11, interested, sell, 40, excellent, best\n",
      "Topic 09: armenian, armenians, turkish, turkey, armenia, turks, genocide, greek, soviet, azerbaijan, russian, muslim, argic, serdar, people, war, azeri, killed, population, children\n",
      "Topic 10: game, team, games, players, season, hockey, play, win, league, teams, nhl, player, baseball, runs, detroit, toronto, fans, pitching, best, playoffs\n",
      "Topic 11: card, video, monitor, bus, vga, cards, drivers, color, driver, memory, ram, mode, graphics, bit, board, pc, ati, vlb, system, vesa\n",
      "Topic 12: car, cars, engine, dealer, miles, bike, speed, driving, front, price, tires, buy, ford, insurance, honda, oil, rear, owner, bought, toyota\n",
      "Topic 13: window, motif, application, server, manager, display, program, widget, problem, code, screen, xterm, set, x11r5, running, user, sun, colormap, mouse, color\n",
      "Topic 14: space, nasa, shuttle, launch, data, station, orbit, program, earth, moon, lunar, gov, research, science, sci, cost, satellite, center, information, system\n",
      "Topic 15: edu, com, cs, ftp, article, university, email, internet, pub, david, send, available, mit, list, address, uiuc, apr, sun, cc, netcom\n",
      "Topic 16: israel, israeli, jews, arab, jewish, arabs, peace, lebanon, lebanese, israelis, war, adam, palestinian, palestinians, land, state, anti, palestine, attacks, syria\n",
      "Topic 17: know, anybody, need, program, something, sure, help, appreciated, wanted, want, happen, heard, source, going, things, far, sorry, kind, really, sites\n",
      "Topic 18: get, want, need, going, try, number, rid, buy, call, source, way, phone, work, life, question, keep, sell, 800, ask, getting\n",
      "Topic 19: people, government, gun, law, right, guns, state, rights, crime, laws, against, federal, police, amendment, weapons, person, country, control, case, believe\n",
      "Topic 20: think, lot, something, wrong, really, way, science, try, objective, agree, see, mean, morality, moral, bit, better, saying, pretty, true, thinking\n"
     ]
    }
   ],
   "source": [
    "# show topic descriptors\n",
    "def get_descriptor( terms, H, topic_index, top ):\n",
    "    top_indices = np.argsort( H[topic_index,:] )[::-1]\n",
    "    top_terms = []\n",
    "    for term_index in top_indices[0:top]:\n",
    "        top_terms.append( terms[term_index] )\n",
    "    return top_terms\n",
    "\n",
    "descriptors = []\n",
    "for topic_index in range(k):\n",
    "    descriptors.append( get_descriptor( terms, H, topic_index, 20 ) )\n",
    "    str_descriptor = \", \".join( descriptors[topic_index] )\n",
    "    print(\"Topic %02d: %s\" % ( topic_index+1, str_descriptor ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Все темы кроме 08 кажутся нормальными"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01. *******\n",
      "*******  This is somewhat long, but pleas read it!!!!!!!!!!!!!!!!!\n",
      "*******\n",
      "\n",
      "\n",
      "\n",
      "Boy am i glad \n",
      "02. Accounts of Anti-Armenian Human Rights Violations in Azerbaijan #007\n",
      "                 Prelude to Cur\n",
      "03. Long time, no see.\n",
      "\n",
      "\t\t\tAndreas\n",
      "\n",
      "-- \n",
      "\n",
      "\t\tAndreas - Siperian Sirri   Siberian Stint\n",
      "04. Accounts of Anti-Armenian Human Right Violations in Azerbaijan #008 Part A\n",
      "                 Prelude \n",
      "05. Accounts of Anti-Armenian Human Right Violations in Azerbaijan #008 Part B\n",
      "                 Prelude \n",
      "06. Sixteen days I had put off test driving the Honda ST1100.  Finally,\n",
      "the 17th was a Saturday without \n",
      "07. Accounts of Anti-Armenian Human Right Violations in Azerbaijan #012\n",
      "                 Prelude to Curr\n",
      "08. Accounts of Anti-Armenian Human Right Violations in Azerbaijan #013\n",
      "                 Prelude to Curr\n",
      "09. I need some advice on having someone ride pillion with me on my 750 Ninja.\n",
      "   This will be the the f\n",
      "10. GREAT post Martin.  Very informative, well-balanced, and humanitarian\n",
      "without neglecting the need fo\n",
      "11. As promised, below is a personal critique of a Pressure Point Massager \n",
      "I recently bought from the S\n",
      "12. Accounts of Anti-Armenian Human Right Violations in Azerbaijan #011\n",
      "                 Prelude to Curr\n",
      "13. Hello,  I am not sure if this is the right conference to ask this\n",
      "question, however, Here I go..  I \n",
      "14. Anyone know what would cause my IIcx to not turn on when I hit the keyboard\n",
      "switch?  The one in the \n",
      "15. [..]\n",
      "\n",
      "\n",
      "Hello.  Firstly, what do you exactly mean by \"fundamentalist\"?  I will\n",
      "for the time being ass\n",
      "16. Ah, so you finally found a use for that super slo-mo and frame advance\n",
      "other than scrutinizing \"Soro\n",
      "17. I've been a Giants season ticket holder for years and never really complained\n",
      "about the old ballyard\n",
      "18. I need some advice on having someone ride pillion with me on my 750 Ninja.\n",
      "This will be the the firs\n",
      "19. Let us not become weary in doing good, for at the proper time we will reap\n",
      "a harvest if we do not gi\n",
      "20. I can not believe the way this thread on candida(yeast) has progressed.\n",
      "Steve Dyer and I have been e\n"
     ]
    }
   ],
   "source": [
    "def get_top_snippets( all_snippets, W, topic_index, top ):\n",
    "    top_indices = np.argsort( W[:,topic_index] )[::-1]\n",
    "    top_snippets = []\n",
    "    for doc_index in top_indices[0:top]:\n",
    "        top_snippets.append( all_snippets[doc_index] )\n",
    "    return top_snippets\n",
    "\n",
    "topic_snippets = get_top_snippets( snippets, W, 0, 20 )\n",
    "for i, snippet in enumerate(topic_snippets):\n",
    "    print(\"%02d. %s\" % ( (i+1), snippet ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['articles-model-nmf-k20.pkl']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump((W,H,terms,snippets), \"articles-model-nmf-k%02d.pkl\" % k) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic coherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22/22 [02:04<00:00,  5.67s/it]\n"
     ]
    }
   ],
   "source": [
    "kmin, kmax = 4, 25\n",
    "\n",
    "topic_models = []\n",
    "for k in tqdm(range(kmin,kmax+1)):\n",
    "    model = NMF( init=\"nndsvd\", n_components=k ) \n",
    "    W = model.fit_transform( A )\n",
    "    H = model.components_    \n",
    "    topic_models.append( (k,W,H) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model has 8460 terms\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import gensim\n",
    "\n",
    "\n",
    "class TokenGenerator:\n",
    "    def __init__( self, documents, stopwords ):\n",
    "        self.documents = documents\n",
    "        self.stopwords = stopwords\n",
    "        self.tokenizer = re.compile( r\"(?u)\\b\\w\\w+\\b\" )\n",
    "\n",
    "    def __iter__( self ):\n",
    "        for doc in self.documents:\n",
    "            tokens = []\n",
    "            for tok in self.tokenizer.findall( doc ):\n",
    "                if tok in self.stopwords:\n",
    "                    tokens.append( \"<stopword>\" )\n",
    "                elif len(tok) >= 2:\n",
    "                    tokens.append( tok )\n",
    "            yield tokens\n",
    "            \n",
    "\n",
    "docgen = TokenGenerator( raw_documents, custom_stop_words )\n",
    "w2v_model = gensim.models.Word2Vec(docgen, size=500, min_count=20, sg=1)\n",
    "print( \"Model has %d terms\" % len(w2v_model.wv.vocab) )\n",
    "w2v_model.save(\"w2v-model.bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TC-W2C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:10: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K=04: Coherence=0.4896\n",
      "K=05: Coherence=0.5074\n",
      "K=06: Coherence=0.5248\n",
      "K=07: Coherence=0.5378\n",
      "K=08: Coherence=0.5361\n",
      "K=09: Coherence=0.5452\n",
      "K=10: Coherence=0.5378\n",
      "K=11: Coherence=0.5502\n",
      "K=12: Coherence=0.5566\n",
      "K=13: Coherence=0.5592\n",
      "K=14: Coherence=0.5568\n",
      "K=15: Coherence=0.5564\n",
      "K=16: Coherence=0.5608\n",
      "K=17: Coherence=0.5520\n",
      "K=18: Coherence=0.5734\n",
      "K=19: Coherence=0.5659\n",
      "K=20: Coherence=0.5718\n",
      "K=21: Coherence=0.5814\n",
      "K=22: Coherence=0.5700\n",
      "K=23: Coherence=0.5656\n",
      "K=24: Coherence=0.5876\n",
      "K=25: Coherence=0.5739\n"
     ]
    }
   ],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "\n",
    "def calculate_coherence( w2v_model, term_rankings ):\n",
    "    overall_coherence = 0.0\n",
    "    for topic_index in range(len(term_rankings)):\n",
    "        # check each pair of terms\n",
    "        pair_scores = []\n",
    "        for pair in combinations( term_rankings[topic_index], 2 ):\n",
    "            pair_scores.append( w2v_model.similarity(pair[0], pair[1]) )\n",
    "        # get the mean for all pairs in this topic\n",
    "        topic_score = sum(pair_scores) / len(pair_scores)\n",
    "        overall_coherence += topic_score\n",
    "    # get the mean score across all topics\n",
    "    return overall_coherence / len(term_rankings)\n",
    "\n",
    "\n",
    "k_values_TC = []\n",
    "coherences_TC = []\n",
    "for (k,W,H) in topic_models:\n",
    "    # Get all of the topic descriptors - the term_rankings, based on top 20 terms\n",
    "    term_rankings = []\n",
    "    for topic_index in range(k):\n",
    "        term_rankings.append( get_descriptor( terms, H, topic_index, 20 ) )\n",
    "    # Now calculate the coherence based on our Word2vec model\n",
    "    k_values_TC.append( k )\n",
    "    coherences_TC.append( calculate_coherence( w2v_model, term_rankings ) )\n",
    "    print(\"K=%02d: Coherence=%.4f\" % ( k, coherences_TC[-1] ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UCI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "text_index = defaultdict(set)\n",
    "for i, text in enumerate(docgen):\n",
    "        for token in text:\n",
    "            text_index[token].add(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "kol_texts = 11314\n",
    "e=10**(-12) \n",
    "#pmi=log(p(w1&w2)/p(w1)*p(w2))\n",
    "def PMI (word1, word2):\n",
    "    p_w1andw2 = len(text_index[word1]&text_index[word2])/kol_texts\n",
    "    pmi = np.log((p_w1andw2+e)/((len(text_index[word1])/kol_texts)*(len(text_index[word2])/kol_texts)))\n",
    "    return pmi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K=04: Coherence=1.6736\n",
      "K=05: Coherence=1.7949\n",
      "K=06: Coherence=1.7834\n",
      "K=07: Coherence=1.7371\n",
      "K=08: Coherence=1.7235\n",
      "K=09: Coherence=1.7122\n",
      "K=10: Coherence=1.6875\n",
      "K=11: Coherence=1.7134\n",
      "K=12: Coherence=1.7182\n",
      "K=13: Coherence=1.7361\n",
      "K=14: Coherence=1.7685\n",
      "K=15: Coherence=1.7976\n",
      "K=16: Coherence=1.8415\n",
      "K=17: Coherence=1.7162\n",
      "K=18: Coherence=1.9092\n",
      "K=19: Coherence=1.7043\n",
      "K=20: Coherence=1.6904\n",
      "K=21: Coherence=1.7154\n",
      "K=22: Coherence=1.5251\n",
      "K=23: Coherence=1.7551\n",
      "K=24: Coherence=1.8261\n",
      "K=25: Coherence=1.6497\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def C_UCI (k, term_rankings):\n",
    "    overall_coherence = 0.0\n",
    "    for topic_index in range(len(term_rankings)):\n",
    "        # check each pair of terms\n",
    "        pair_scores = []\n",
    "        for pair in combinations(term_rankings[topic_index], 2):\n",
    "            pair_scores.append(PMI(pair[0], pair[1]))\n",
    "        # get the mean for all pairs in this topic\n",
    "        topic_score = (2/(len(term_rankings[topic_index])*(len(term_rankings[topic_index])-1)))*sum(pair_scores)\n",
    "        overall_coherence += topic_score\n",
    "    # get the mean score across all topics\n",
    "    return overall_coherence/k\n",
    "\n",
    "\n",
    "k_values_UCI = []\n",
    "coherences_UCI = []\n",
    "for (k,W,H) in topic_models:\n",
    "    # Get all of the topic descriptors - the term_rankings, based on top 20 terms\n",
    "    term_rankings = []\n",
    "    for topic_index in range(k):\n",
    "        term_rankings.append( get_descriptor( terms, H, topic_index, 20 ) )\n",
    "    # Now calculate the coherence based on our Word2vec model\n",
    "    k_values_UCI.append( k )\n",
    "    coherences_UCI.append( C_UCI( k, term_rankings ) )\n",
    "    print(\"K=%02d: Coherence=%.4f\" % ( k, coherences[-1] ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UMass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_p(word1, word2):\n",
    "    p_w1andw2 = len(text_index[word1]&text_index[word2])/kol_texts\n",
    "    log = np.log((p_w1andw2+e)/(len(text_index[word1])/kol_texts))\n",
    "    return log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K=04: Coherence=-1.7901\n",
      "K=05: Coherence=-1.7679\n",
      "K=06: Coherence=-1.8462\n",
      "K=07: Coherence=-1.8884\n",
      "K=08: Coherence=-1.8920\n",
      "K=09: Coherence=-2.0094\n",
      "K=10: Coherence=-2.0164\n",
      "K=11: Coherence=-2.0597\n",
      "K=12: Coherence=-2.0848\n",
      "K=13: Coherence=-2.1058\n",
      "K=14: Coherence=-2.0806\n",
      "K=15: Coherence=-2.0689\n",
      "K=16: Coherence=-2.0389\n",
      "K=17: Coherence=-2.0678\n",
      "K=18: Coherence=-1.9959\n",
      "K=19: Coherence=-2.3022\n",
      "K=20: Coherence=-2.2380\n",
      "K=21: Coherence=-2.2848\n",
      "K=22: Coherence=-2.3986\n",
      "K=23: Coherence=-2.1357\n",
      "K=24: Coherence=-2.1767\n",
      "K=25: Coherence=-2.3180\n"
     ]
    }
   ],
   "source": [
    "def C_UMass (k, term_rankings):\n",
    "    overall_coherence = 0.0\n",
    "    for topic_index in range(len(term_rankings)):\n",
    "        # check each pair of terms\n",
    "        pair_scores = []\n",
    "        for pair in combinations(term_rankings[topic_index], 2):\n",
    "            pair_scores.append(log_p(pair[0], pair[1]))\n",
    "        # get the mean for all pairs in this topic\n",
    "        topic_score = (2/(len(term_rankings[topic_index])*(len(term_rankings[topic_index])-1)))*sum(pair_scores)\n",
    "        overall_coherence += topic_score\n",
    "    # get the mean score across all topics\n",
    "    return overall_coherence/k\n",
    "\n",
    "\n",
    "k_values = []\n",
    "coherences = []\n",
    "for (k,W,H) in topic_models:\n",
    "    # Get all of the topic descriptors - the term_rankings, based on top 20 terms\n",
    "    term_rankings = []\n",
    "    for topic_index in range(k):\n",
    "        term_rankings.append( get_descriptor( terms, H, topic_index, 20 ) )\n",
    "    # Now calculate the coherence based on our Word2vec model\n",
    "    k_values.append( k )\n",
    "    coherences.append( C_UMass( k, term_rankings ) )\n",
    "    print(\"K=%02d: Coherence=%.4f\" % ( k, coherences[-1] ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.externals import joblib\n",
    "from sklearn import metrics\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "A,terms,snippets = joblib.load(\"articles-tfidf.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model\n",
    "k = 20\n",
    "model_LDA = LatentDirichletAllocation(n_components=k, max_iter=5,\n",
    "                                learning_method='batch',\n",
    "                                learning_offset=50.,\n",
    "                                random_state=1)\n",
    "\n",
    "W = model_LDA.fit_transform( A )\n",
    "H = model_LDA.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 01: msg, food, car, helmet, think, cars, foods, chinese, good, something, time, long, eat, taste, allergic, know, get, reaction, sugar, ago\n",
      "Topic 02: god, people, government, think, law, believe, christian, religion, truth, see, true, life, know, jesus, church, christians, things, right, really, way\n",
      "Topic 03: card, bus, mouse, thanks, windows, drivers, vlb, driver, know, hi, video, ati, gateway, diamond, cica, ftp, isa, desktop, ram, version\n",
      "Topic 04: adam, captain, israel, pov, harvard, behavior, traded, jews, das, switzerland, tonight, israeli, judaism, blacks, word, deletion, energy, agree, finland, sue\n",
      "Topic 05: sale, scsi, shipping, drive, offer, price, condition, disks, 00, deleted, best, model, brand, asking, tube, sell, interested, 25, stuff, selling\n",
      "Topic 06: people, armenian, war, think, armenians, time, space, know, go, turkish, president, government, islam, nasa, really, before, see, get, against, women\n",
      "Topic 07: windows, thanks, know, get, system, file, program, key, need, please, software, drive, help, problem, files, dos, mail, information, work, available\n",
      "Topic 08: get, think, team, good, time, game, know, people, 10, gun, back, see, right, games, players, way, go, play, power, edu\n",
      "Topic 09: pgp, please, steve, thanks, mail, sas, email, upenn, turkish, go, armenia, keith, quakers, ivy, faq, know, keller, archie, champs, replies\n",
      "Topic 10: test, wanted, temp, pointer, favorite, sig, wuarchive, wustl, exactly, oil, bring, israel, air, vw, flat, gonna, yo, happy, routine, cars\n",
      "Topic 11: israel, israeli, jews, jewish, know, arab, people, arabs, land, palestinian, think, get, palestinians, fire, honda, nazi, right, anti, state, israelis\n",
      "Topic 12: pain, ground, com, mask, lib, get, lcs, mit, oil, uh, export, kent, mine, cat, fault, agree, go, dream, cheers, people\n",
      "Topic 13: borland, joke, interior, tx, __, lh, peter, ml, yep, david, gee, old, edu, radius, probe, fool, mm, mw, fm, timing\n",
      "Topic 14: god, jesus, bible, christ, sin, christians, life, john, faith, lord, christian, father, son, spirit, believe, christianity, sex, resurrection, sabbath, holy\n",
      "Topic 15: bat, virginia, david, gt, keyboard, hacker, bmw, watch, se, click, cheers, tony, acs, r5, nearby, errors, cruel, think, jon, menu\n",
      "Topic 16: ax, methodology, disease, ye, huh, faces, scientific, cylinder, science, diseases, vacuum, german, col, apostles, iran, adobe, language, germany, death, times\n",
      "Topic 17: pitt, surrender, gordon, banks, geb, cadre, skepticism, shameful, intellect, n3jxp, chastity, dsl, soon, edu, engine, detector, radar, colormap, car, widget\n",
      "Topic 18: simms, simm, fpu, nubus, gl, window, cpu, vram, icon, lc, pds, tires, expose, dram, amp, crashes, widgets, cubs, interrupt, pixmap\n",
      "Topic 19: braves, address, game, baseball, wings, rbi, espn, please, leafs, good, get, offense, hit, know, er, list, hall, heard, mail, runs\n",
      "Topic 20: eisa, dog, system, wheel, dma, shuttle, gear, shaft, inputs, objective, sound, isa, file, swap, morality, vl, dogs, know, telnet, export\n"
     ]
    }
   ],
   "source": [
    "# show topic descriptors\n",
    "def get_descriptor( terms, H, topic_index, top ):\n",
    "    top_indices = np.argsort( H[topic_index,:] )[::-1]\n",
    "    top_terms = []\n",
    "    for term_index in top_indices[0:top]:\n",
    "        top_terms.append( terms[term_index] )\n",
    "    return top_terms\n",
    "\n",
    "descriptors = []\n",
    "for topic_index in range(k):\n",
    "    descriptors.append( get_descriptor( terms, H, topic_index, 20 ) )\n",
    "    str_descriptor = \", \".join( descriptors[topic_index] )\n",
    "    print(\"Topic %02d: %s\" % ( topic_index+1, str_descriptor ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Здесь все вроде бы нормальные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01. I've never seen a speedometer-reading model. Are you sure? Who makes\n",
      "them? Consider the difficulty o\n",
      "02. So, how did you guys *learn* this? Is it something you were\n",
      "born with, or did you make horrible grin\n",
      "03. hey... I'm pretty new to the wonderful world of motorcycles... I just\n",
      "bought\n",
      "a used 81 Kaw KZ650 CSR\n",
      "04. I've seen a film of it, my memory may be faulty, but as I\n",
      "remember it the vehicle was slightly over \n",
      "05. Apparently that last post was a little hasy, since I\n",
      "called around to more places and got quotes for\n",
      "06. In the Air Force world at least, the crisis escalates when scale\n",
      "models of the plane in question (i.\n",
      "07. Some recent postings remind me that I had read about risks \n",
      "associated with the barbecuing of foods,\n",
      "08. Several years ago GM was having trouble with the rings sticking on the\n",
      "5.7 diesel.  They traced a ca\n",
      "09. Yeah, I'm sure that our lab would love a ride (he's the type that sticks his\n",
      "head out car windows) b\n",
      "10. One of those \"morning, just getting the coffee in me\" thoughts:\n",
      " \n",
      " Waving at other bikers makes more\n",
      "11. Did he ever really convert?  He married a Jewish woman, but I've never\n",
      "heard him say he converted.  \n",
      "12. A friend has what is apparently a fairly minor case of Crohn's\n",
      "disease.\n",
      "\n",
      "But she can't seem to eat c\n",
      "13. Some birds require constant management for survival.  Pointing a sensor at\n",
      "the sun, even when powere\n",
      "14. And the reason that the Soviet Union couldn't achieve the ideal of pure\n",
      "communism was the hostility \n",
      "15. I have a new doctor who gave me a prescription today for something called \n",
      "Septra DS.  He said it ma\n",
      "16. Two things:\n",
      "\n",
      "\t1. Read your own posts. I was agreeing with Bob. No correction\n",
      "\t   needed.\n",
      "\n",
      "\t2. Don't \n",
      "17. When your helmetted nogin hits an immoveabe object, there are only four things\n",
      "to dissipate the ener\n",
      "18. I'm told that corn allergy is fairly common.  My wife has it and it seems\n",
      "to be exacerbated if sugar\n",
      "19. <apparently you're not a woman - my husband hates the auto door locks\n",
      "<feels safer in a car that loc\n",
      "20. If your primary concern is protecting the passenger in the event of a\n",
      "crash, have him or her fitted \n"
     ]
    }
   ],
   "source": [
    "def get_top_snippets( all_snippets, W, topic_index, top ):\n",
    "    top_indices = np.argsort( W[:,topic_index] )[::-1]\n",
    "    top_snippets = []\n",
    "    for doc_index in top_indices[0:top]:\n",
    "        top_snippets.append( all_snippets[doc_index] )\n",
    "    return top_snippets\n",
    "\n",
    "topic_snippets = get_top_snippets( snippets, W, 0, 20 )\n",
    "for i, snippet in enumerate(topic_snippets):\n",
    "    print(\"%02d. %s\" % ( (i+1), snippet ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/22 [00:00<?, ?it/s]\n",
      "Exception in thread Thread-27:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Anaconda3\\lib\\threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Anaconda3\\lib\\site-packages\\tqdm\\_monitor.py\", line 63, in run\n",
      "    for instance in self.tqdm_cls._instances:\n",
      "  File \"C:\\Anaconda3\\lib\\_weakrefset.py\", line 60, in __iter__\n",
      "    for itemref in self.data:\n",
      "RuntimeError: Set changed size during iteration\n",
      "\n",
      "100%|██████████| 22/22 [11:35<00:00, 31.61s/it]\n"
     ]
    }
   ],
   "source": [
    "kmin, kmax = 4, 25\n",
    "\n",
    "topic_models_LDA = []\n",
    "for k in tqdm(range(kmin,kmax+1)):\n",
    "    model_LDA = LatentDirichletAllocation(n_components=k, max_iter=5,\n",
    "                                learning_method='batch',\n",
    "                                learning_offset=50.,\n",
    "                                random_state=1)\n",
    "    W = model_LDA.fit_transform( A )\n",
    "    H = model_LDA.components_    \n",
    "    topic_models_LDA.append( (k,W,H) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TC-W2V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:10: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K=04: Coherence=0.3628\n",
      "K=05: Coherence=0.3756\n",
      "K=06: Coherence=0.3761\n",
      "K=07: Coherence=0.4266\n",
      "K=08: Coherence=0.4489\n",
      "K=09: Coherence=0.4450\n",
      "K=10: Coherence=0.4379\n",
      "K=11: Coherence=0.4356\n",
      "K=12: Coherence=0.4208\n",
      "K=13: Coherence=0.4301\n",
      "K=14: Coherence=0.4721\n",
      "K=15: Coherence=0.4527\n",
      "K=16: Coherence=0.4552\n",
      "K=17: Coherence=0.4633\n",
      "K=18: Coherence=0.4236\n",
      "K=19: Coherence=0.4590\n",
      "K=20: Coherence=0.4572\n",
      "K=21: Coherence=0.4458\n",
      "K=22: Coherence=0.4476\n",
      "K=23: Coherence=0.4370\n",
      "K=24: Coherence=0.4591\n",
      "K=25: Coherence=0.4904\n"
     ]
    }
   ],
   "source": [
    "k_values = []\n",
    "coherences = []\n",
    "for (k,W,H) in topic_models_LDA:\n",
    "    # Get all of the topic descriptors - the term_rankings, based on top 20 terms\n",
    "    term_rankings = []\n",
    "    for topic_index in range(k):\n",
    "        term_rankings.append( get_descriptor( terms, H, topic_index, 20 ) )\n",
    "    # Now calculate the coherence based on our Word2vec model\n",
    "    k_values.append( k )\n",
    "    coherences.append( calculate_coherence( w2v_model, term_rankings ) )\n",
    "    print(\"K=%02d: Coherence=%.4f\" % ( k, coherences[-1] ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UCI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K=04: Coherence=0.5513\n",
      "K=05: Coherence=0.2235\n",
      "K=06: Coherence=0.1926\n",
      "K=07: Coherence=0.1547\n",
      "K=08: Coherence=0.0893\n",
      "K=09: Coherence=-0.3922\n",
      "K=10: Coherence=-0.8595\n",
      "K=11: Coherence=-0.5742\n",
      "K=12: Coherence=-2.0172\n",
      "K=13: Coherence=-0.3151\n",
      "K=14: Coherence=-0.0449\n",
      "K=15: Coherence=-1.6435\n",
      "K=16: Coherence=-0.4039\n",
      "K=17: Coherence=-0.6589\n",
      "K=18: Coherence=-1.6557\n",
      "K=19: Coherence=-0.5044\n",
      "K=20: Coherence=-2.1187\n",
      "K=21: Coherence=-1.4054\n",
      "K=22: Coherence=-1.5418\n",
      "K=23: Coherence=-1.5550\n",
      "K=24: Coherence=-1.6933\n",
      "K=25: Coherence=-2.7008\n"
     ]
    }
   ],
   "source": [
    "k_values = []\n",
    "coherences = []\n",
    "for (k,W,H) in topic_models_LDA:\n",
    "    # Get all of the topic descriptors - the term_rankings, based on top 20 terms\n",
    "    term_rankings = []\n",
    "    for topic_index in range(k):\n",
    "        term_rankings.append( get_descriptor( terms, H, topic_index, 20 ) )\n",
    "    # Now calculate the coherence based on our Word2vec model\n",
    "    k_values.append( k )\n",
    "    coherences.append( C_UCI( k, term_rankings ) )\n",
    "    print(\"K=%02d: Coherence=%.4f\" % ( k, coherences[-1] ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UMass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K=04: Coherence=-2.3103\n",
      "K=05: Coherence=-2.8102\n",
      "K=06: Coherence=-3.0890\n",
      "K=07: Coherence=-3.2976\n",
      "K=08: Coherence=-3.5205\n",
      "K=09: Coherence=-4.1707\n",
      "K=10: Coherence=-4.6696\n",
      "K=11: Coherence=-4.3995\n",
      "K=12: Coherence=-6.1289\n",
      "K=13: Coherence=-4.0859\n",
      "K=14: Coherence=-3.9018\n",
      "K=15: Coherence=-5.8684\n",
      "K=16: Coherence=-4.3315\n",
      "K=17: Coherence=-4.8149\n",
      "K=18: Coherence=-5.6995\n",
      "K=19: Coherence=-4.4619\n",
      "K=20: Coherence=-6.4247\n",
      "K=21: Coherence=-5.5797\n",
      "K=22: Coherence=-5.7909\n",
      "K=23: Coherence=-5.7011\n",
      "K=24: Coherence=-5.9114\n",
      "K=25: Coherence=-7.3287\n"
     ]
    }
   ],
   "source": [
    "k_values = []\n",
    "coherences = []\n",
    "for (k,W,H) in topic_models_LDA:\n",
    "    # Get all of the topic descriptors - the term_rankings, based on top 20 terms\n",
    "    term_rankings = []\n",
    "    for topic_index in range(k):\n",
    "        term_rankings.append( get_descriptor( terms, H, topic_index, 20 ) )\n",
    "    # Now calculate the coherence based on our Word2vec model\n",
    "    k_values.append( k )\n",
    "    coherences.append( C_UMass( k, term_rankings ) )\n",
    "    print(\"K=%02d: Coherence=%.4f\" % ( k, coherences[-1] ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "A,terms,snippets = joblib.load(\"articles-tfidf.pkl\")\n",
    "\n",
    "# create the model\n",
    "k = 20\n",
    "model_LSA = TruncatedSVD(n_components=k ) \n",
    "\n",
    "W = model_LSA.fit_transform( A )\n",
    "H = model_LSA.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 01: know, people, get, think, good, time, thanks, god, please, see, want, way, need, right, system, windows, problem, edu, something, really\n",
      "Topic 02: god, people, jesus, think, believe, bible, christian, christians, life, government, see, christ, faith, law, right, israel, religion, jews, fact, against\n",
      "Topic 03: geb, cadre, pitt, dsl, n3jxp, chastity, shameful, edu, intellect, skepticism, surrender, gordon, banks, soon, get, team, game, key, probably, car\n",
      "Topic 04: god, jesus, edu, thanks, geb, cadre, dsl, n3jxp, chastity, skepticism, shameful, intellect, pitt, gordon, surrender, banks, windows, soon, please, bible\n",
      "Topic 05: key, chip, encryption, government, clipper, keys, file, public, program, law, escrow, information, windows, algorithm, files, people, security, system, window, nsa\n",
      "Topic 06: drive, scsi, system, key, disk, card, drives, chip, ide, hard, controller, god, bus, floppy, problem, bit, dos, encryption, clipper, apple\n",
      "Topic 07: windows, file, window, dos, files, program, problem, think, running, team, win, run, game, version, screen, drivers, manager, server, application, driver\n",
      "Topic 08: god, key, 00, game, team, games, edu, chip, 10, jesus, sale, encryption, keys, clipper, com, season, 11, players, 15, 20\n",
      "Topic 09: 00, israel, edu, armenian, space, sale, armenians, jews, people, com, israeli, turkish, 10, 000, war, shipping, offer, nasa, 20, 15\n",
      "Topic 10: car, bike, get, good, 00, god, cars, price, engine, miles, sale, window, condition, buy, dealer, speed, monitor, sell, offer, oil\n",
      "Topic 11: card, video, monitor, drivers, people, cards, vga, bus, driver, israel, chip, government, color, ati, windows, mode, jews, ram, memory, vlb\n",
      "Topic 12: space, think, card, nasa, edu, com, video, bus, science, mac, data, scsi, shuttle, system, graphics, launch, article, monitor, available, get\n",
      "Topic 13: window, israel, problem, israeli, jews, get, display, application, motif, server, armenian, manager, widget, screen, key, armenians, arab, thanks, god, set\n",
      "Topic 14: edu, com, get, window, please, mail, car, think, email, israel, right, address, send, mouse, cs, try, go, want, list, scsi\n",
      "Topic 15: window, think, 00, people, please, sale, shipping, software, offer, motif, windows, problem, interested, application, price, display, manager, system, condition, screen\n",
      "Topic 16: get, people, 00, armenian, armenians, turkish, gun, want, god, going, government, armenia, go, turkey, know, window, right, guns, need, turks\n",
      "Topic 17: know, 00, file, israel, edu, get, sale, files, good, shipping, want, jews, offer, price, think, ftp, israeli, key, software, graphics\n",
      "Topic 18: get, israel, space, system, need, windows, please, mail, game, god, sale, israeli, offer, jews, shipping, games, power, address, printer, sell\n",
      "Topic 19: please, key, time, armenian, armenians, think, problem, dos, space, email, back, turkish, jesus, address, windows, know, someone, bit, send, post\n",
      "Topic 20: know, space, system, monitor, window, right, please, god, problem, israel, mail, address, file, 00, law, nasa, list, go, drive, dos\n"
     ]
    }
   ],
   "source": [
    "# show topic descriptors\n",
    "def get_descriptor( terms, H, topic_index, top ):\n",
    "    top_indices = np.argsort( H[topic_index,:] )[::-1]\n",
    "    top_terms = []\n",
    "    for term_index in top_indices[0:top]:\n",
    "        top_terms.append( terms[term_index] )\n",
    "    return top_terms\n",
    "\n",
    "descriptors = []\n",
    "for topic_index in range(k):\n",
    "    descriptors.append( get_descriptor( terms, H, topic_index, 20 ) )\n",
    "    str_descriptor = \", \".join( descriptors[topic_index] )\n",
    "    print(\"Topic %02d: %s\" % ( topic_index+1, str_descriptor ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вроде бы всё нормальное"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01. Accounts of Anti-Armenian Human Rights Violations in Azerbaijan #007\n",
      "                 Prelude to Cur\n",
      "02. Accounts of Anti-Armenian Human Right Violations in Azerbaijan #008 Part B\n",
      "                 Prelude \n",
      "03. Accounts of Anti-Armenian Human Right Violations in Azerbaijan #012\n",
      "                 Prelude to Curr\n",
      "04. *******\n",
      "*******  This is somewhat long, but pleas read it!!!!!!!!!!!!!!!!!\n",
      "*******\n",
      "\n",
      "\n",
      "\n",
      "Boy am i glad \n",
      "05. I am not an expert in the cryptography science, but some basic things\n",
      "seem evident to me, things whi\n",
      "06. Archive-name: graphics/resources-list/part3\n",
      "Last-modified: 1993/04/17\n",
      "\n",
      "\n",
      "Computer Graphics Resource L\n",
      "07. THE WHITE HOUSE\n",
      "\n",
      "                    Office of the Press Secretary\n",
      "_________________________________\n",
      "08. GREAT post Martin.  Very informative, well-balanced, and humanitarian\n",
      "without neglecting the need fo\n",
      "09. Accounts of Anti-Armenian Human Right Violations in Azerbaijan #008 Part A\n",
      "                 Prelude \n",
      "10. Accounts of Anti-Armenian Human Right Violations in Azerbaijan #013\n",
      "                 Prelude to Curr\n",
      "11. THE WHITE HOUSE\n",
      "\n",
      "                    Office of the Press Secretary\n",
      "_________________________________\n",
      "12. Archive-name: net-privacy/part3\n",
      "Last-modified: 1993/3/3\n",
      "Version: 2.1\n",
      "\n",
      "\n",
      "NOTES on ANONYMITY on the INT\n",
      "13. Archive-name: atheism/faq\n",
      "Alt-atheism-archive-name: faq\n",
      "Last-modified: 5 April 1993\n",
      "Version: 1.1\n",
      "\n",
      "  \n",
      "14. THE WHITE HOUSE\n",
      "\n",
      "                    Office of the Press Secretary\n",
      "_________________________________\n",
      "15. A listmember (D Andrew Killie, I think) wrote, in response to the\n",
      "suggestion that genocide may somet\n",
      "16. Archive-name: x-faq/part1\n",
      "Last-modified: 1993/04/04\n",
      "\n",
      "This article and several following contain the \n",
      "17. Thanks for posting this and making it available. This post will be LONG, I will\n",
      "comment on most of i\n",
      "18. :   I have normal procomm plus for dos, but I've been considering buying\n",
      ": the windows version....it\n",
      "19. Accounts of Anti-Armenian Human Right Violations in Azerbaijan #011\n",
      "                 Prelude to Curr\n",
      "20. Here is a press release from the White House.\n",
      "\n",
      " President Clinton's Remarks On Waco With Q/A\n",
      " To: Na\n"
     ]
    }
   ],
   "source": [
    "def get_top_snippets( all_snippets, W, topic_index, top ):\n",
    "    top_indices = np.argsort( W[:,topic_index] )[::-1]\n",
    "    top_snippets = []\n",
    "    for doc_index in top_indices[0:top]:\n",
    "        top_snippets.append( all_snippets[doc_index] )\n",
    "    return top_snippets\n",
    "\n",
    "topic_snippets = get_top_snippets( snippets, W, 0, 20 )\n",
    "for i, snippet in enumerate(topic_snippets):\n",
    "    print(\"%02d. %s\" % ( (i+1), snippet ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22/22 [00:09<00:00,  2.26it/s]\n"
     ]
    }
   ],
   "source": [
    "kmin, kmax = 4, 25\n",
    "\n",
    "topic_models_LSA = []\n",
    "for k in tqdm(range(kmin,kmax+1)):\n",
    "    model_LSA = TruncatedSVD(n_components=k ) \n",
    "    W = model_LSA.fit_transform( A )\n",
    "    H = model_LSA.components_    \n",
    "    topic_models_LSA.append( (k,W,H) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TC-W2V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:10: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K=04: Coherence=0.4570\n",
      "K=05: Coherence=0.4477\n",
      "K=06: Coherence=0.4448\n",
      "K=07: Coherence=0.4459\n",
      "K=08: Coherence=0.4251\n",
      "K=09: Coherence=0.4205\n",
      "K=10: Coherence=0.4104\n",
      "K=11: Coherence=0.4201\n",
      "K=12: Coherence=0.4120\n",
      "K=13: Coherence=0.4051\n",
      "K=14: Coherence=0.3988\n",
      "K=15: Coherence=0.4031\n",
      "K=16: Coherence=0.3863\n",
      "K=17: Coherence=0.4020\n",
      "K=18: Coherence=0.3875\n",
      "K=19: Coherence=0.3753\n",
      "K=20: Coherence=0.3634\n",
      "K=21: Coherence=0.3703\n",
      "K=22: Coherence=0.3665\n",
      "K=23: Coherence=0.3688\n",
      "K=24: Coherence=0.3677\n",
      "K=25: Coherence=0.3619\n"
     ]
    }
   ],
   "source": [
    "k_values = []\n",
    "coherences = []\n",
    "for (k,W,H) in topic_models_LSA:\n",
    "    # Get all of the topic descriptors - the term_rankings, based on top 20 terms\n",
    "    term_rankings = []\n",
    "    for topic_index in range(k):\n",
    "        term_rankings.append( get_descriptor( terms, H, topic_index, 20 ) )\n",
    "    # Now calculate the coherence based on our Word2vec model\n",
    "    k_values.append( k )\n",
    "    coherences.append( calculate_coherence( w2v_model, term_rankings ) )\n",
    "    print(\"K=%02d: Coherence=%.4f\" % ( k, coherences[-1] ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UCI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K=04: Coherence=-0.0416\n",
      "K=05: Coherence=0.4171\n",
      "K=06: Coherence=0.2203\n",
      "K=07: Coherence=0.3439\n",
      "K=08: Coherence=0.3578\n",
      "K=09: Coherence=0.2518\n",
      "K=10: Coherence=0.2019\n",
      "K=11: Coherence=0.1800\n",
      "K=12: Coherence=0.2567\n",
      "K=13: Coherence=0.2335\n",
      "K=14: Coherence=0.0831\n",
      "K=15: Coherence=0.2466\n",
      "K=16: Coherence=0.0191\n",
      "K=17: Coherence=0.3055\n",
      "K=18: Coherence=0.0278\n",
      "K=19: Coherence=0.0146\n",
      "K=20: Coherence=-0.1050\n",
      "K=21: Coherence=-0.1524\n",
      "K=22: Coherence=0.0650\n",
      "K=23: Coherence=-0.0515\n",
      "K=24: Coherence=0.1157\n",
      "K=25: Coherence=-0.0663\n"
     ]
    }
   ],
   "source": [
    "k_values = []\n",
    "coherences = []\n",
    "for (k,W,H) in topic_models_LSA:\n",
    "    # Get all of the topic descriptors - the term_rankings, based on top 20 terms\n",
    "    term_rankings = []\n",
    "    for topic_index in range(k):\n",
    "        term_rankings.append( get_descriptor( terms, H, topic_index, 20 ) )\n",
    "    # Now calculate the coherence based on our Word2vec model\n",
    "    k_values.append( k )\n",
    "    coherences.append( C_UCI( k, term_rankings ) )\n",
    "    print(\"K=%02d: Coherence=%.4f\" % ( k, coherences[-1] ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UMass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K=04: Coherence=-3.4338\n",
      "K=05: Coherence=-2.9857\n",
      "K=06: Coherence=-3.2409\n",
      "K=07: Coherence=-3.1335\n",
      "K=08: Coherence=-3.1244\n",
      "K=09: Coherence=-3.2838\n",
      "K=10: Coherence=-3.3239\n",
      "K=11: Coherence=-3.3794\n",
      "K=12: Coherence=-3.3317\n",
      "K=13: Coherence=-3.3424\n",
      "K=14: Coherence=-3.5213\n",
      "K=15: Coherence=-3.3453\n",
      "K=16: Coherence=-3.5345\n",
      "K=17: Coherence=-3.2622\n",
      "K=18: Coherence=-3.5987\n",
      "K=19: Coherence=-3.5137\n",
      "K=20: Coherence=-3.6215\n",
      "K=21: Coherence=-3.7458\n",
      "K=22: Coherence=-3.4668\n",
      "K=23: Coherence=-3.6273\n",
      "K=24: Coherence=-3.4355\n",
      "K=25: Coherence=-3.6392\n"
     ]
    }
   ],
   "source": [
    "k_values = []\n",
    "coherences = []\n",
    "for (k,W,H) in topic_models_LSA:\n",
    "    # Get all of the topic descriptors - the term_rankings, based on top 20 terms\n",
    "    term_rankings = []\n",
    "    for topic_index in range(k):\n",
    "        term_rankings.append( get_descriptor( terms, H, topic_index, 20 ) )\n",
    "    # Now calculate the coherence based on our Word2vec model\n",
    "    k_values.append( k )\n",
    "    coherences.append( C_UMass( k, term_rankings ) )\n",
    "    print(\"K=%02d: Coherence=%.4f\" % ( k, coherences[-1] ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NMF \n",
    "\n",
    "TC-W2V: K=24: Coherence=0.5876 |\n",
    "UCI: K=18: Coherence=1.9092 |\n",
    "Umass: K=05: Coherence=-1.7679 \n",
    "\n",
    "LDA\n",
    "\n",
    "TC-W2V: K=25: Coherence=0.4904 |\n",
    "UCI: K=04: Coherence=0.5513 |\n",
    "Umass: K=04: Coherence=-2.3103 \n",
    "\n",
    "\n",
    "LSA\n",
    "\n",
    "TC-W2V: K=04: Coherence=0.4570 |\n",
    "UCI: K=05: Coherence=0.4171 |\n",
    "Umass: K=05: Coherence=-2.9857"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Самая лучшая по всем метрикам - NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x14881a88e80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(13,7))\n",
    "# create the line plot\n",
    "plt.plot( k_values_TC, coherences_TC, 'r', label = 'TC' )\n",
    "plt.plot( k_values_UCI, coherences_UCI, 'b', label = 'UCI' )\n",
    "plt.plot( k_values_UMass, coherences_UMass, 'g', label = 'UMass' )\n",
    "plt.xticks(k_values_TC)\n",
    "plt.xticks(k_values_UMass)\n",
    "plt.xticks(k_values_UCI)\n",
    "plt.xlabel(\"Number of Topics\")\n",
    "plt.ylabel(\"Mean Coherence\")\n",
    "# add the points\n",
    "plt.scatter( k_values_UMass, coherences_UMass, s=50, color ='g')\n",
    "plt.scatter( k_values_TC, coherences_TC, s=50, color ='r')\n",
    "plt.scatter( k_values_UCI, coherences_UCI, s=50, color ='b')\n",
    "# find and annotate the maximum point on the plot\n",
    "ymax = max(coherences_TC)\n",
    "ymax2 = max(coherences_UCI)\n",
    "ymax3 = max(coherences_UMass)\n",
    "xpos1 = coherences_TC.index(ymax)\n",
    "xpos2 = coherences_UCI.index(ymax2)\n",
    "xpos3 = coherences_UMass.index(ymax3)\n",
    "best_k1 = k_values_TC[xpos1]\n",
    "best_k2 = k_values_UCI[xpos2]\n",
    "best_k3 = k_values_UMass[xpos3]\n",
    "plt.annotate( \"k=%d\" % best_k1, xy=(best_k1, ymax), xytext=(best_k1, ymax), textcoords=\"offset points\", fontsize=16)\n",
    "plt.annotate( \"k=%d\" % best_k2, xy=(best_k2, ymax2), xytext=(best_k2, ymax2), textcoords=\"offset points\", fontsize=16)\n",
    "plt.annotate( \"k=%d\" % best_k3, xy=(best_k3, ymax3), xytext=(best_k3, ymax3), textcoords=\"offset points\", fontsize=16)\n",
    "plt.legend()\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 01: good, time, see, really, back, go, got, something, thing, going, take, long, bike, want, way, sure, right, little, stuff, things\n",
      "Topic 02: thanks, advance, hi, info, looking, help, appreciated, anybody, information, find, appreciate, wondering, greatly, someone, lot, ftp, graphics, site, hello, software\n",
      "Topic 03: geb, cadre, dsl, n3jxp, chastity, shameful, pitt, intellect, skepticism, surrender, gordon, banks, soon, edu, blood, patients, probably, medical, weight, disease\n",
      "Topic 04: god, jesus, bible, christ, believe, faith, christian, christians, church, life, sin, truth, heaven, lord, hell, belief, man, christianity, love, father\n",
      "Topic 05: key, chip, encryption, clipper, keys, escrow, algorithm, government, public, nsa, security, secure, phone, des, bit, encrypted, secret, chips, number, privacy\n",
      "Topic 06: drive, drives, disk, hard, floppy, cd, ide, boot, controller, internal, cable, rom, hd, switch, tape, power, external, format, disks, computer\n",
      "Topic 07: windows, file, dos, files, program, version, ftp, directory, pc, running, os, microsoft, run, nt, drivers, driver, ini, win, zip, disk\n",
      "Topic 08: 10, 11, 12, 15, 16, 14, 17, 20, 13, 25, 24, 18, 23, 21, 30, 19, 27, period, 22, 000\n",
      "Topic 09: armenian, armenians, turkish, turkey, armenia, turks, genocide, greek, soviet, russian, azerbaijan, muslim, argic, serdar, people, war, azeri, killed, population, kurds\n",
      "Topic 10: game, team, games, players, hockey, season, play, win, teams, nhl, league, player, baseball, runs, best, detroit, fans, pitching, toronto, playoffs\n",
      "Topic 11: card, video, monitor, vga, drivers, cards, driver, color, bus, mode, graphics, ati, vesa, diamond, ram, svga, vlb, board, colors, 16\n",
      "Topic 12: car, cars, engine, dealer, miles, bike, speed, driving, front, tires, ford, buy, price, insurance, oil, honda, rear, owner, bought, toyota\n",
      "Topic 13: window, motif, application, manager, display, server, program, widget, code, xterm, screen, set, x11r5, user, top, colormap, sun, color, client, running\n",
      "Topic 14: space, nasa, shuttle, launch, program, station, orbit, data, earth, moon, gov, lunar, research, science, sci, cost, satellite, center, information, mission\n",
      "Topic 15: edu, com, cs, ftp, article, university, internet, pub, david, email, mit, available, uiuc, cc, netcom, colorado, sun, zip, apr, au\n",
      "Topic 16: israel, israeli, jews, arab, jewish, arabs, peace, lebanon, lebanese, israelis, war, adam, palestinian, palestinians, land, state, anti, palestine, attacks, syria\n",
      "Topic 17: get, want, need, going, try, number, rid, buy, source, way, call, phone, work, life, question, keep, sell, getting, ask, 800\n",
      "Topic 18: know, anybody, need, program, something, sure, wanted, help, want, appreciated, happen, heard, source, interested, going, far, sorry, things, kind, really\n",
      "Topic 19: people, government, gun, law, right, guns, state, rights, crime, laws, against, federal, police, amendment, weapons, person, country, control, case, militia\n",
      "Topic 20: 00, sale, shipping, offer, price, condition, asking, 50, best, interested, excellent, sell, 20, brand, includes, old, original, disks, obo, manual\n",
      "Topic 21: scsi, ide, controller, bus, pc, isa, bit, mac, devices, interface, data, sec, chip, transfer, dma, hd, fast, port, vlb, speed\n",
      "Topic 22: system, problem, apple, mac, memory, software, problems, computer, machine, work, screen, running, hardware, duo, port, printer, modem, fine, mouse, fix\n",
      "Topic 23: please, mail, email, address, send, list, post, interested, reply, someone, thank, looking, mailing, contact, information, tell, fax, hello, respond, replies\n",
      "Topic 24: think, lot, something, science, wrong, way, try, really, agree, mean, see, bit, objective, people, morality, pretty, moral, saying, thinking, better\n"
     ]
    }
   ],
   "source": [
    "W_TC = topic_models[k-kmin][1]\n",
    "H = topic_models[k-kmin][2]\n",
    "for topic_index in range(best_k1):\n",
    "    descriptor = get_descriptor( terms, H, topic_index, 20 )\n",
    "    str_descriptor = \", \".join( descriptor )\n",
    "    print(\"Topic %02d: %s\" % ( topic_index+1, str_descriptor ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UCI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 01: good, time, see, really, back, go, got, something, thing, going, take, long, bike, want, way, sure, right, little, stuff, things\n",
      "Topic 02: thanks, advance, hi, info, looking, help, appreciated, anybody, information, find, appreciate, wondering, greatly, someone, lot, ftp, graphics, site, hello, software\n",
      "Topic 03: geb, cadre, dsl, n3jxp, chastity, shameful, pitt, intellect, skepticism, surrender, gordon, banks, soon, edu, blood, patients, probably, medical, weight, disease\n",
      "Topic 04: god, jesus, bible, christ, believe, faith, christian, christians, church, life, sin, truth, heaven, lord, hell, belief, man, christianity, love, father\n",
      "Topic 05: key, chip, encryption, clipper, keys, escrow, algorithm, government, public, nsa, security, secure, phone, des, bit, encrypted, secret, chips, number, privacy\n",
      "Topic 06: drive, drives, disk, hard, floppy, cd, ide, boot, controller, internal, cable, rom, hd, switch, tape, power, external, format, disks, computer\n",
      "Topic 07: windows, file, dos, files, program, version, ftp, directory, pc, running, os, microsoft, run, nt, drivers, driver, ini, win, zip, disk\n",
      "Topic 08: 10, 11, 12, 15, 16, 14, 17, 20, 13, 25, 24, 18, 23, 21, 30, 19, 27, period, 22, 000\n",
      "Topic 09: armenian, armenians, turkish, turkey, armenia, turks, genocide, greek, soviet, russian, azerbaijan, muslim, argic, serdar, people, war, azeri, killed, population, kurds\n",
      "Topic 10: game, team, games, players, hockey, season, play, win, teams, nhl, league, player, baseball, runs, best, detroit, fans, pitching, toronto, playoffs\n",
      "Topic 11: card, video, monitor, vga, drivers, cards, driver, color, bus, mode, graphics, ati, vesa, diamond, ram, svga, vlb, board, colors, 16\n",
      "Topic 12: car, cars, engine, dealer, miles, bike, speed, driving, front, tires, ford, buy, price, insurance, oil, honda, rear, owner, bought, toyota\n",
      "Topic 13: window, motif, application, manager, display, server, program, widget, code, xterm, screen, set, x11r5, user, top, colormap, sun, color, client, running\n",
      "Topic 14: space, nasa, shuttle, launch, program, station, orbit, data, earth, moon, gov, lunar, research, science, sci, cost, satellite, center, information, mission\n",
      "Topic 15: edu, com, cs, ftp, article, university, internet, pub, david, email, mit, available, uiuc, cc, netcom, colorado, sun, zip, apr, au\n",
      "Topic 16: israel, israeli, jews, arab, jewish, arabs, peace, lebanon, lebanese, israelis, war, adam, palestinian, palestinians, land, state, anti, palestine, attacks, syria\n",
      "Topic 17: get, want, need, going, try, number, rid, buy, source, way, call, phone, work, life, question, keep, sell, getting, ask, 800\n",
      "Topic 18: know, anybody, need, program, something, sure, wanted, help, want, appreciated, happen, heard, source, interested, going, far, sorry, things, kind, really\n"
     ]
    }
   ],
   "source": [
    "W = topic_models[k-kmin][1]\n",
    "H = topic_models[k-kmin][2]\n",
    "for topic_index in range(best_k2):\n",
    "    descriptor = get_descriptor( terms, H, topic_index, 20 )\n",
    "    str_descriptor = \", \".join( descriptor )\n",
    "    print(\"Topic %02d: %s\" % ( topic_index+1, str_descriptor ) )    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UMass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 01: good, time, see, really, back, go, got, something, thing, going, take, long, bike, want, way, sure, right, little, stuff, things\n",
      "Topic 02: thanks, advance, hi, info, looking, help, appreciated, anybody, information, find, appreciate, wondering, greatly, someone, lot, ftp, graphics, site, hello, software\n",
      "Topic 03: geb, cadre, dsl, n3jxp, chastity, shameful, pitt, intellect, skepticism, surrender, gordon, banks, soon, edu, blood, patients, probably, medical, weight, disease\n",
      "Topic 04: god, jesus, bible, christ, believe, faith, christian, christians, church, life, sin, truth, heaven, lord, hell, belief, man, christianity, love, father\n",
      "Topic 05: key, chip, encryption, clipper, keys, escrow, algorithm, government, public, nsa, security, secure, phone, des, bit, encrypted, secret, chips, number, privacy\n"
     ]
    }
   ],
   "source": [
    "W = topic_models[k-kmin][1]\n",
    "H = topic_models[k-kmin][2]\n",
    "for topic_index in range(best_k3):\n",
    "    descriptor = get_descriptor( terms, H, topic_index, 20 )\n",
    "    str_descriptor = \", \".join( descriptor )\n",
    "    print(\"Topic %02d: %s\" % ( topic_index+1, str_descriptor ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Самое лучшее у ТС"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reccomendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsRecommender:\n",
    "    def __init__(self, stopwords=custom_stop_words):\n",
    "        self.texts=texts\n",
    "        self.stopwords = stopwords\n",
    "        self.vectorizer = TfidfVectorizer(min_df=20, stop_words=self.stopwords)\n",
    "        self.nearn = NearestNeighbors(metric='cosine')\n",
    "        self.model = NMF(init=\"nndsvd\", n_components=24, random_state=74)\n",
    "\n",
    "    def train(self, texts):\n",
    "        A = self.vectorizer.fit_transform(self.texts)\n",
    "        W = self.model.fit_transform(A)\n",
    "        self.nearn.fit(W)\n",
    "    \n",
    "    def recommend(self, text_sample, k):\n",
    "        news = []\n",
    "        vect = self.vectorizer.transform([text_sample.lower()])\n",
    "        nmf = self.model.transform(vect)\n",
    "        texts_ids = self.nearn.kneighbors(nmf)[1][0]\n",
    "        for i in texts_ids:\n",
    "            news.append(raw_documents[i])\n",
    "        return news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = NewsRecommender()\n",
    "r.train(raw_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тестовый текст из изначального набора текстов, которые были загружены в начале. Текст про религию и большинство из рекомендованных как-то связаны с темой религии."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "slight semantical difference.  the lds church does own a heck of\n",
      "a lot however.  they are the largest land holder in missouri\n",
      "(where they think christ will appear at the second coming).\n",
      "-----------------------\n",
      "for example, if it were instinctive not to murder...\n",
      "\n",
      "\n",
      "so, only intelligent beings can be moral, even if the bahavior of other\n",
      "beings mimics theirs?  and, how much emphasis do you place on intelligence?\n",
      "animals of the same species could kill each other arbitarily, but they\n",
      "don't.  are you trying to say that this isn't an act of morality because\n",
      "most animals aren't intelligent enough to think like we do?\n",
      "-----------------------\n",
      "i don't think such tools exist either. in addition, there's no such\n",
      "thing as objective information. all together, it looks like religion\n",
      "and any doctrines could be freely misused to whatever purpose.\n",
      "\n",
      "this all reminds me of descartes' whispering deamon. you can't trust\n",
      "anything. so why bother.\n",
      "\n",
      "cheers,\n",
      "kent\n",
      "-----------------------\n",
      "well, this is alt.atheism.  i hope you arent here to try to convert anyone.\n",
      "\n",
      "\n",
      "many would disagree.\n",
      "\n",
      "[...]\n",
      "\n",
      "well, you shouldn't give any particular book too much weight.  actually,\n",
      "i don't think that any of these statements is correct.  it is more likely\n",
      "that most of jesus' fame was attributed to him after his death by those\n",
      "who had some strong motives...\n",
      "\n",
      "[...]\n",
      "\n",
      "what's a prophecy, and what's so significant about them?\n",
      "\n",
      "\n",
      "i think we understand.\n",
      "\n",
      "\n",
      "well, sell your computer and donate you life to your religion now...\n",
      "don't waste any time.\n",
      "-----------------------\n",
      "peter,\n",
      "\n",
      "i believe this is your most succinct post to date. since you have nothing\n",
      "to say, you say nothing! it's brilliant. did you think of this all by\n",
      "yourself?\n",
      "\n",
      "-marc\n",
      "-----------------------\n"
     ]
    }
   ],
   "source": [
    "res = r.recommend(raw_documents[5], 4)\n",
    "for j in res:\n",
    "    print(j)\n",
    "    print('-----------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь текст из  из bbc news за 2017 год"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bbc_news = 'The year 2017 saw a burst of optimism among American investors and consumers, despite the turmoil sparked by President Trumps first year in office.The President strode from one political storm to the next as he took an axe to many of the policies of the Obama administration, but - on the economic front - the waters could hardly have been smoother.Annual economic growth has been a healthy 3% while the United States stock market is up around 25% on the year.It is true that the dollar is down roughly 10%, which makes the real rise in share prices a little less impressive, but the improved confidence of many Americans is impossible to deny.According to the US economist Irwin Stelzer of the Hudson Institute, it is the low unemployment rate of 4.1% that is the key factor.There are six million unfilled jobs in the United States at the moment, he says.'\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">finally, because there is essentially no possibility of intercepting in\n",
      "   >realtime the scrutable content of communications between stolen instruments,\n",
      "   >there will exist strong motivation to record and archive _all_ communications\n",
      "   >in the network for ex-post-facto scrutiny (once some criminal act is\n",
      "   >discovered, and the instruments involved have been identified).\n",
      "\n",
      "\"all\" is a *very* big number.  the at&t long distance network has\n",
      "around 20,000 t3 trunks (45 mbit/sec), which is on the order of 10**12 bits/sec.\n",
      "that doesn't even count the amount of traffic in the local phone companies,\n",
      "or our long-distance competitors.  it's about 200 exabytes tapes / second,\n",
      "which is pretty large even for the nsa :-)\n",
      "\n",
      "on the other hand, i can easily see them recording the traffic for\n",
      "\"interesting\" people, such as dissidents, suspected criminals,\n",
      "foreign telephone calls, and anybody noticed using encryption.\n",
      "as ken shiriff speculates, recording encrypted traffic will probably\n",
      "be judged not to be an invasion of privacy pretty soon ....\n",
      "-----------------------\n",
      "[...]\n",
      "\n",
      "(the date i have for this is 1-26-93)\n",
      "\n",
      "note clinton's statements about encryption in the 3rd paragraph..  i guess\n",
      "this statement doesen't contradict what you said, though.\n",
      "\n",
      "--- cut here ---\n",
      "-----------------------\n",
      "but, those chips are probably inside a custom chip, (to make it smaller and\n",
      "use less power) and the preset/data pins are not going to be available.\n",
      "it would probably not be ttl but might be cmos \n",
      "(wider operating voltage range), not that the tecnology would make \n",
      "much difference.\n",
      "plus the custom chip would probably be potted (encapsulated with epoxy).\n",
      "good luck.\n",
      "\n",
      "-- \n",
      "--garyl-------------------------------------------------------------------------\n",
      "\t\t\"any shark that gets to be 11 or 12 feet long with \n",
      "\t      300 big teeth can be considered dangerous\" - 'shark bowl '92'\n",
      "-----------------------\n",
      "i don't have the wiretap statute handy.  but here's what the law says\n",
      "on pen registers.  this is all from title 18 of the u.s. code.  note\n",
      "how vague s. 3125(a)(1)(b) is....  i haven't had a chance to check\n",
      "out 50 u.s.c. 1801 yet.\n",
      "\n",
      "----\n",
      "\n",
      "18 usc  s. 3121 pen registers (as of 4/93)\n",
      "\n",
      "\n",
      "s. 3121. general prohibition on  pen register  and trap and trace device\n",
      "use; exception\n",
      "\n",
      "   (a) in general. except as provided in this section, no person may\n",
      "install or use a  pen register  or a trap and trace device without first\n",
      "obtaining a court order under section 3123 of this title or under the\n",
      "foreign intelligence surveillance act of 1978 (50 u.s.c. 1801 et seq.).\n",
      "\n",
      ".....\n",
      "\n",
      "s. 3125.  emergency  pen register  and trap and trace device\n",
      "installation\n",
      "\n",
      "   (a) notwithstanding any other provision of this chapter , any\n",
      "investigative or law enforcement officer, specially designated by the\n",
      "attorney general, the deputy attorney general, the associate attorney\n",
      "general, any assistant attorney general, any acting assistant attorney\n",
      "general, or any deputy assistant attorney general, or by the principal\n",
      "prosecuting attorney of any state or subdivision thereof acting pursuant\n",
      "to a statute of that state, who reasonably determines that--\n",
      "\n",
      "   (1) an emergency situation exists that involves--\n",
      "\n",
      "   (a) immediate danger of death or serious bodily injury to any person;\n",
      "or\n",
      "\n",
      "   (b) conspiratorial activities characteristic of organized crime,\n",
      "\n",
      "   that requires the installation and use of a  pen register  or a trap\n",
      "and trace device before an order authorizing such installation and use\n",
      "can, with due diligence, be obtained, and\n",
      "\n",
      "   (2) there are grounds upon which an order could be entered under this\n",
      "chapter to authorize such installation and use \"may have installed and\n",
      "use a  pen register  or trap and trace device if, within forty-eight\n",
      "hours after the installation has occurred, or begins to occur, an order\n",
      "approving the installation or use is issued in accordance with section\n",
      "3123 of this title.\"\n",
      "\n",
      "   (b) in the absence of an authorizing order, such use shall\n",
      "immediately terminate when the information sought is obtained, when the\n",
      "application for the order is denied or when forty-eight hours have\n",
      "lapsed since the installation of the  pen register  or trap and trace\n",
      "device, whichever is earlier.\n",
      "-----------------------\n",
      "i must respectfully disagree with this assertion, brad.  the government is\n",
      "notoriously sloppy with physical, communications, and information security.  they\n",
      "can't keep their computers safe, and they're \"trying\".  read \"dea is\n",
      "not adequately protecting national security information\" [gao/imtec 92-31] for an\n",
      "excellent example of what i'm talking about.\n",
      "\n",
      "private sector organizations tend to be even more lax in their security measures.\n",
      "i believe that the escrow organizations will be penetrated by foreign\n",
      "intelligence services within months, if not weeks, of their selection.  private\n",
      "organizations that lack the resources of a full-fleged intelligence service will\n",
      "take longer - perhaps on the order of one to two years.  nonetheless, the\n",
      "penetrations will take place, without question.\n",
      "-----------------------\n"
     ]
    }
   ],
   "source": [
    "res2 = r.recommend(bbc_news, 4)\n",
    "for j2 in res2:\n",
    "    print(j2)\n",
    "    print('-----------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
